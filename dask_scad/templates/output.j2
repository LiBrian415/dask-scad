#@ type: compute
{%- if parents %}
#@ parents:
{%- for p in parents %}
#@    - {{p.get_id()}}
{%- endfor %}
{%- endif %}
{%- if corunning %}
#@ corunning:
{%- for c in corunning %}
#@    {{c.get_id()}}:
#@        trans: {{c.get_id()}}
#@        type: rdma
{%- endfor %}
{%- endif %}

import base64
import cloudpickle
import redis
import uuid

limit = 1000 * 1000

def main(params, action):
    scad_output = {{ output }}

    # Input Processing
    cache = dict()
    for _, v in params.items():
        meta = cloudpickle.loads(base64.b64decode(v[0]['meta']))
        if meta['type'] == 'df':
            (k, t, a, s) = meta['df']

            trans = action.get_transport(t, 'rdma')
            trans.reg(1024 * 1024 * 64)

            for i in range(0, s, limit):
                ts = min(limit, s-i)
                trans.read(ts, a+i, i)

            df = cloudpickle.loads(trans.buf[:s])

            # OutputElement will return the entire df so just loaded the blocks
            # instead of using RemoteBlocks
            blks = meta['blocks']
            data_blks = []
            for b in blks:
                (a, s) = b['remote_meta']['meta']

                for i in range(0, s, limit):
                    ts = min(limit, s-i)
                    trans.read(ts, a+i, i)

                data_blks.append(cloudpickle.loads(trans.buf[:s]))

            df._data.blocks = tuple(data_blks)
            cache[k] = df
        else:
            (k, t, a, s) = meta['mem']

            trans = action.get_transport(t, 'rdma')
            trans.reg(s)

            for i in range(0, s, limit):
                ts = min(limit, s-i)
                trans.read(ts, a+i, i)

            cache[k] = cloudpickle.loads(trans.buf[:s])

    # Output Processing
    # This is a stupid way to do this and doesn't really allow for easy support
    # for other mediums, but oh well...
    if scad_output['type'] != 'redis':
        raise

    output = dict()
    r = redis.Redis(host=scad_output['meta']['host'], port=scad_output['meta']['port'])
    for k, v in cache.items():
        serialized = cloudpickle.dumps(v)
        key = base64.b64encode(uuid.uuid4().bytes).decode('ascii')
        r.set(key, serialized)
        output[k] = key

    return {'output': base64.b64encode(cloudpickle.dumps(output)).decode('ascii')}

