#@ type: compute
{%- if parents %}
#@ parents:
{%- for p in parents %}
#@    - {{p.get_id()}}
{%- endfor %}
{%- endif %}
{%- if dependents %}
#@ dependents:
{%- for d in dependents %}
#@    - {{d.get_id()}}
{%- endfor %}
{%- endif %}
{%- if corunning %}
#@ corunning:
{%- for c in corunning %}
#@    {{c.get_id()}}:
#@        trans: {{c.get_id()}}
#@        type: rdma
{%- endfor %}
{%- endif %}

import base64
import cloudpickle
import pandas as pd

from dask.core import _execute_task
from pandas.core.internals.blocks import RemoteBlock

limit = 1000 * 1000

def main(params, action):
    key = cloudpickle.loads({{ key }})
    output = {{ '\'' + output + '\''}}
    computation = {{ computation }}

    # Input Processing
    cache = dict()
    for _, v in params.items():
        meta = cloudpickle.loads(base64.b64decode(v[0]['meta']))
        if meta['type'] == 'df':
            (k, t, a, s) = meta['df']

            trans = action.get_transport(t, 'rdma')
            trans.reg(1024 * 1024 * 64)

            for i in range(0, s, limit):
                ts = min(limit, s-i)
                trans.read(ts, a+i, i)

            df = cloudpickle.loads(trans.buf[:s])

            blks = meta['blocks']
            remote_blks = []
            for b in blks:
                remote_blk = RemoteBlock(bp=b['bp'],
                                         shape=b['shape'],
                                         ndim=b['ndim'],
                                         remote_meta=b['remote_meta'],
                                         transport=trans)
                remote_blks.append(remote_blk)

            df._data.blocks = tuple(remote_blks)
            cache[k] = df
        else:
            (k, t, a, s) = meta['mem']

            trans = action.get_transport(t, 'rdma')
            trans.reg(s)

            for i in range(0, s, limit):
                ts = min(limit, s-i)
                trans.read(ts, a+i, i)

            cache[k] = cloudpickle.loads(trans.buf[:s])

    # Execute Task
    res = _execute_task(cloudpickle.loads(computation), cache)

    # Output Processing
    if isinstance(res, pd.DataFrame):
        df = res
        trans = action.get_transport(output, 'rdma')
        trans.reg(1024 * 1024 * 64)

        remote_blocks_meta = []
        address = 0

        for blk in df._data.blocks:
            if isinstance(blk, RemoteBlock):
                blk._load()
                blk = blk.delegate

            serialized = cloudpickle.dumps(blk)
            size = len(serialized)
            remote_meta = {
                'bp': blk._mgr_locs.orig_val,
                'shape': blk.shape,
                'ndim': blk.ndim,
                'remote_meta': {'type': 'scad', 'meta': (address, size)}
            }
            remote_blocks_meta.append(remote_meta)

            trans.buf[address:address+size] = serialized
            address += size

        df._data.blocks = ()
        serialized_df = cloudpickle.dumps(df)
        size = len(serialized_df)
        df_loc = (key, output, address, size)

        trans.buf[address:address+size] = serialized_df
        address += size

        # Write entire transport to memory element
        for i in range(0, address, limit):
            ts = min(limit, address-i)
            trans.write(ts, i, i)

        meta = {
            'type': 'df',
            'df': df_loc,
            'blocks': remote_blocks_meta
        }
    else:
        serialized = cloudpickle.dumps(res)
        address = 0
        size = len(serialized)

        trans = action.get_transport(output, 'rdma')
        trans.reg(size)
        trans.buf[:] = serialized

        for i in range(0, size, limit):
            ts = min(limit, size-i)
            trans.write(ts, address+i, i)

        meta = {
            'type': 'misc',
            'mem': (key, output, address, size)
        }

    return {'meta': base64.b64encode(cloudpickle.dumps(meta)).decode('ascii')}

